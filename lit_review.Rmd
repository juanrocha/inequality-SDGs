---
title: 'Literature review: Inequality and the biosphere'
author: "Juan Rocha"
date: "September 2021"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
      smooth_control: true
    toc_depth: 3
    fig_caption: true
    fig_width: 8
    fig_height: 5
    highlight: tango
    self_contained: true
    theme: "paper"
    
  html_notebook:
    toc: yes
    toc_float: yes
    highlight: tango
    code_folding: hide
    theme:
      bootswatch: cosmo
      code_font:
        google: Fira Code
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE, warning = FALSE, message = FALSE, echo = TRUE,
  fig.width = 8, fig.height = 5
)


library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
library(lda)
library(naniar)
library(plotly)
library(patchwork)

theme_set(theme_light())
```

Below I read the data and organize the data by most citations with the data that Emmy provided (N=86430 papers):
```{r message = FALSE, warning=FALSE}


fls <- list.files(path = "data/lit_review/")

dat_xls <- paste0("data/lit_review/", str_subset(fls, ".xlsx")) %>% 
    map(readxl::read_excel) %>% 
    map(janitor::clean_names) %>% 
    map(function(x) x %>% select(abstract, title, year, doi, cited_by) %>% as_tibble()) %>% 
    map(function(x) x %>% mutate(across(.cols = everything(), .fns = as.character))) %>% 
    bind_rows()

dat_csv  <- paste0("data/lit_review/", str_subset(fls, ".csv")) %>% # head() %>% 
    map(read.csv) %>% 
    map(janitor::clean_names) %>% 
    map(function(x) x %>% select(abstract, title, year, doi, cited_by) %>% as_tibble()) %>% 
    map(function(x) x %>% mutate(across(.cols = everything(), .fns = as.character))) %>% 
    bind_rows()

dat <- bind_rows(dat_csv, dat_xls)
rm(dat_csv, dat_xls)

dat %>% 
  select(title, year, cited_by, doi, abstract) %>% 
  arrange(desc(cited_by))
```

## Topic model

I have done the topic modelling on the data Emmy downloaded. Here some prelim results:

```{r}
## This does not compile to HTML due to memory, I compute stuff offline and add figs manually
# load("data/210903_topic_models_gibbs.RData")
# dtm

```

Now we have a document term matrix with 73622 documents (abstracts) and 164098 words, after cleaning for common words, stop words and numbers. I pre-computed the topic models outside this notebook because it takes some time (days) and memory, so not suitable for computing at running time. Here are some visualizations of the preliminary results: 

```{r}

# k <- c(25,50,100,250,500)
# df_topic_number <- tibble(
#     topic_number = k,
#     entropy = map_dbl (topicNumber.TM, function (x)
#         mean(apply(posterior(x)$topics, 1, function (z) - sum(z * log(z)))) # maximize Entropy
#     ),
#     alpha = map_dbl(topicNumber.TM, slot, "alpha"),
#     log_lik = map_dbl(topicNumber.TM, logLik) #,  #maximize loglik
#     #perplexity = map_dbl(topicNumber.TM, perplexity) #minimize perplexity
# )
# 
# df_stats <- tibble(
#     model = names(lapply(tset.TM, logLik)),
#     loglik = as.numeric(lapply(tset.TM, logLik)), #maximize loglik
#     entropy = lapply (tset.TM, function (x) 
#         mean(apply(posterior(x)$topics,
#                    1, function (z) - sum(z * log(z))))) %>% as.numeric()#maximize ENTROPY
#     
# )
# 
# 
# perp <-  lapply(tset.TM[c(1,2,4)], perplexity)
# perp$Gibbs <- NA
# # pretty names:
# df_stats$model <- c("VEM alpha", "VEM fixed", "Gibbs", "CTM")
# 
# g1 <- df_stats %>%
#     add_column(perplexity = as.numeric(perp[c(1,2,4,3)])) %>% #minimize perplexity
#     pivot_longer(cols = 2:4, names_to = "measure", values_to = "value") %>%
#     ggplot(aes(model, value)) + 
#     geom_col() + 
#     # scale_y_continuous(labels = scales::label_scientific) +
#     facet_wrap(.~measure, scales = "free_y") +
#     labs(x = "Algorithm", y = "Value", tag = "A") +
#     theme_light(base_size = 15) + 
#     theme(axis.text.x = element_text(size = 5))
# 
# g2 <- df_topic_number %>%
#     # mutate(alpha_log = log10(alpha)) %>%
#     pivot_longer(cols = 2:last_col(), names_to = "measure", values_to = "value") %>%
#     # filter(measure != "alpha") %>%
#     ggplot(aes(as.factor(topic_number), value)) +
#     geom_col() + 
#     # scale_y_continuous(labels = scales::label_scientific) +
#     labs(x = "Number of topics", y = "Value", tag = "B") +
#     facet_wrap(.~measure, scales = "free", ncol = 4, nrow = 1) +
#     theme_light(base_size = 15)
# 
# g1/g2

```

![Algorithm selection](figures/topicstats.png)

The best algorithm should maximize entropy and the log-likelihood while reducing perplexity. The latter cannot be calculated for Gibbs sampling, but it seems to be the best performing algorithm. The optimal number of topics is 100, where the log-likelihood starts to decay again. For now I will be using 50 topics and Gibbs sampling for the rest of the analysis (reason: simpler visualizations).


A topic is the (posterior) probability distributions of words being likely to be representing together certain hidden structures (topics) in the text. All words in the vocabulary (N = 164098 words) have a probability of explaining a topic, here I'm only showing the top 10 words per topic: 

```{r fig.width=8, fig.height=10}
# df_topics25 <- tidy(topicNumber.TM[[3]], matrix = "beta")
# 
# g_25 <- df_topics25 %>%
#     group_by(topic) %>%
#     top_n(25, beta) %>%
#     ungroup() %>%
#     arrange(topic, - beta) %>%
#     mutate(term = reorder_within(term, beta, topic)) %>%
#     ggplot(aes(term, beta)) +
#     geom_col(aes(fill = as.factor(topic)),show.legend = FALSE) +
#     coord_flip() +
#     scale_x_reordered() +
#     labs(y = "Probability of word explaining the topic", x = "Word ranking") +
#     facet_wrap(.~ topic, scales = "free_y", ncol = 10) +
#     theme_light(base_size = 5) +
#     theme(axis.text.x = element_text(size = 5))
# 
# 
# g_25
# 
# ggsave(
#     filename = "topics_100.png",
#     path = "figures/",
#     plot = g_25,
#     device = "png",
#     width = 10, height = 12, units = "in",
#     dpi = 400,
#     bg = "white"
# )
```
![Topics](figures/topics_50.png)
Now let's see how topics change over time:

```{r}
# df_documents <- tidy(topicNumber.TM[[2]], matrix = "gamma")
# 
# papers_yr <- dat %>%
#     #filter(year > 2000, year < 2021) %>%
#     group_by(year) %>%
#     tally() %>% rename(total_papers = n)
# 
# 
# g_stack <-  df_documents %>% 
#     left_join(
#         dat %>% 
#             select(abstract, title, year) %>%
#             filter(!is.na(abstract)) %>%
#             unique(),
#         by = c("document" = "title") ) %>%
#     mutate(year = as.numeric(year)) %>% 
#     filter(year > 2000, year <2021) %>%
#     group_by(topic, year) %>% 
#     summarize(gamma_yr_tp = sum(gamma), .groups = "drop") %>% 
#     group_by(year) %>%
#     mutate(total = sum(gamma_yr_tp)) %>%
#     ungroup() %>% 
#     mutate(proportion = gamma_yr_tp / total,
#            topic = as.factor(topic)) %>%
#     ggplot(aes(x = year, y = proportion, group = topic)) +
#     geom_area(aes(fill = topic, color = topic), 
#               position = "stack", show.legend = FALSE, alpha = 0.5, size = 0.25) +
#     labs(y = "Relative proportion of topics", x = "Year") +
#     theme_classic(base_size = 15) 
# 
# g_stack
# 
# ggsave(
#     filename = "topics_time.png",
#     path = "figures/",
#     plot = g_stack,
#     device = "png",
#     width = 4, height = 3, units = "in",
#     dpi = 400,
#     bg = "white"
# )
```
![Topics over time](figures/topics_time.png)
There is no particular dominance of a topic over another, each topic has received relatively similar attention in academic productivity. If you're interested in learning more about the method or applications, you can learn more [here](https://arxiv.org/abs/2012.14312) or the references therein. 

## Co-author networks 

**This is based on the old analysis with 2000 papers, please ignore for now**

Below some code to construct the network. Note however that it's very slow. With over 4500 authors we have a network of co-authorship with over 232k links. The network alone is over 200Mb in RAM so at this stage any expensive computation is not worthed until we get the final data set (e.g. centrality, community detection, etc). I leave the graph here as demonstration, but in the explorative phase it is perhaps more useful tables with high impact papers per topic, and key authors per topic to follow up with the qualitative analysis.

```{r fig.width=8}
# library(network)
# authors <- dat %>% 
#     select(authors, doi) %>% 
#     mutate(authors = str_split(authors, pattern = ", ")) %>% 
#     unnest(cols = authors) %>% 
#     mutate(presence = 1)
# 
# mat <- authors %>% unique() %>% 
#     pivot_wider(names_from = authors, values_from = presence, values_fill = 0) %>% 
#     select(-doi) %>%
#     as.matrix()
# 
# mat <- t(mat) %*% (mat)
# 
# 
# df_authors <- as_tibble(mat, rownames = "author") 
# 
# df_authors <- df_authors %>% 
#     pivot_longer(2:last_col(), names_to = "co_author", values_to = "n_papers") %>% 
#     filter(author != co_author, n_papers > 0)
# 
# net <- df_authors %>% 
#     # filtering reduces from 232k links and >4.5k authors to 708 links shared by 233 authors
#     filter(n_papers > 1) %>% 
#     network(., directed = FALSE, matrix.type = "edgelist")
# 
# networkD3::simpleNetwork(
#     Data= df_authors %>% filter(n_papers >1) ,
#     Source = "author", Target = "co_author", 
#     zoom = TRUE, linkColour = "grey", nodeColour = "blue"
# )
# 
# ## free memory
# rm(mat, authors)

```

